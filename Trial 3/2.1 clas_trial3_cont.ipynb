{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wscchan.censtatd/.local/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from pyarrow.feather import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fastai import *\n",
    "from fastai.lm_rnn import *\n",
    "from fastai.text import *\n",
    "import pickle\n",
    "import json\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reading padded indices inputs (298 tokens max)\n",
    "# \"empty container\" --> 401 16 1 1 1... 1\n",
    "trn_clas = np.array(pd.read_feather('seq_trn_trial3.feather'))\n",
    "#val_clas = np.array(pd.read_feather('seq_val_trial3.feather'))\n",
    "#test_clas = np.array(pd.read_feather('seq_test_trial3.feather'))\n",
    "\n",
    "# Reading SubCodes dataframe then converting them into one large array\n",
    "trn_labels = np.squeeze(np.array(pd.read_feather('lbl_trn_trial3.feather')))\n",
    "#val_labels = np.squeeze(np.array(pd.read_feather('lbl_val_trial3.feather')))\n",
    "#test_labels = np.squeeze(np.array(pd.read_feather('lbl_test_trial3.feather')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int to str\n",
    "itos = pd.read_feather('vocab_EN10_CN.feather')['term']\n",
    "# str to int\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "\n",
    "# int to label\n",
    "itol = pd.read_feather('52SubCodes.feather')['lbl']\n",
    "# label to int\n",
    "ltoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itol)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting SubCodes to indices (0 to 51)\n",
    "# \"000\" --> 0; \"040\" --> 1; ...; \"999\" --> 51\n",
    "trn_labels = np.array([ltoi[trn_labels[i]] for i in range(len(trn_labels))])\n",
    "#val_labels = np.array([ltoi[val_labels[i]] for i in range(len(val_labels))])\n",
    "#test_labels = np.array([ltoi[test_labels[i]] for i in range(len(test_labels))])\n",
    "\n",
    "# Number of nominal classes\n",
    "c = 52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back propagation through time, embedding size, hidden unit, hidden layer\n",
    "bptt, em_sz, nh, nl = 70, 400, 1150, 3\n",
    "# Vocab size\n",
    "vs = len(itos)\n",
    "# Optimising function\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "# Batch size\n",
    "bs = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_ds = TextDataset(trn_clas, trn_labels)\n",
    "#val_ds = TextDataset(val_clas, val_labels)\n",
    "#test_ds = TextDataset(test_clas, test_labels)\n",
    "\n",
    "trn_samp = SortishSampler(trn_clas, key=lambda x: len(trn_clas[x]), bs=bs//2)\n",
    "#val_samp = SortSampler(val_clas, key=lambda x: len(val_clas[x]))\n",
    "\n",
    "trn_dl = DataLoader(trn_ds, bs//2, transpose=True, num_workers=1, pad_idx=1, sampler=trn_samp)\n",
    "#val_dl = DataLoader(val_ds, bs, transpose=True, num_workers=1, pad_idx=1, sampler=val_samp)\n",
    "#test_dl = DataLoader(test_ds, bs, transpose=True, num_workers=1, pad_idx=1)\n",
    "\n",
    "md = ModelData('', trn_dl, None)#trn_dl, val_dl)#, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dps = np.array([0.4, 0.5, 0.05, 0.3, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rnn_classifier(bptt, max_seq, n_class, n_tok, emb_sz, n_hid, n_layers, pad_token, layers, drops, bidir=False,\n",
    "                      dropouth=0.3, dropouti=0.5, dropoute=0.1, wdrop=0.5, qrnn=False):\n",
    "    rnn_enc = MultiBatchRNN(bptt, max_seq, n_tok, emb_sz, n_hid, n_layers, pad_token=pad_token, bidir=bidir,\n",
    "                      dropouth=dropouth, dropouti=dropouti, dropoute=dropoute, wdrop=wdrop)\n",
    "    return SequentialRNN(rnn_enc, PoolingLinearClassifier(layers, drops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the RNN model\n",
    "m = get_rnn_classifier(bptt, 20*70, 52, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1,\n",
    "          layers=[em_sz*3, 50, 52], drops=[dps[4], 0.1],\n",
    "          dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimising function\n",
    "opt_fn = partial(optim.Adam, betas=(0.7, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)\n",
    "learn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\n",
    "learn.clip = 25.\n",
    "learn.metrics = [accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "   0.5291\n",
       "   1.2135\n",
       "   0.7796\n",
       "  12.2349\n",
       "   0.5780\n",
       "   2.1629\n",
       "   8.2946\n",
       "   0.7379\n",
       "   0.8337\n",
       "   2.2457\n",
       "   3.9084\n",
       "   0.8359\n",
       "   7.5712\n",
       "   0.9132\n",
       "  69.7656\n",
       "   6.3712\n",
       " 252.5851\n",
       "   6.8233\n",
       "   1.4222\n",
       "   0.9846\n",
       "   5.5943\n",
       "   0.2328\n",
       "   0.5485\n",
       "   1.4861\n",
       "   0.8234\n",
       "   0.7232\n",
       "  28.6206\n",
       "   0.8360\n",
       "   1.7174\n",
       "   2.5297\n",
       "   8.5309\n",
       "   0.7402\n",
       "   1.1919\n",
       "   0.6126\n",
       "   1.3018\n",
       "   0.8539\n",
       "   0.1695\n",
       "   1.4689\n",
       "   1.2719\n",
       "  51.8925\n",
       "   0.8109\n",
       "   2.2017\n",
       "   1.0758\n",
       "   1.5498\n",
       "   0.2823\n",
       "   1.1277\n",
       "   3.3827\n",
       "   4.3563\n",
       "   1.3638\n",
       "   0.2594\n",
       "  79.2611\n",
       "  31.2671\n",
       "[torch.DoubleTensor of size 52x1]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading a custom weight array for the loss function\n",
    "torch.from_numpy(np.array(pd.read_feather('loss_wgt.feather')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.crit = torch.nn.CrossEntropyLoss(weight=torch.from_numpy(np.array(pd.read_feather('loss_wgt.feather'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining learning rates\n",
    "lr = 3e-3\n",
    "lrm = 2.6\n",
    "lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight decay\n",
    "wd = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load_encoder('lm1_enc_EN10_CN')\n",
    "learn.load('clas_2_trial3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(32,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('clas_2_trial3_cont_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('clas_2_trial3_cont_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(32,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('clas_2_trial3_cont_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_clas = np.array(d.read_feather('t2s_df_Aug14.feather'))\n",
    "all_labels = np.squeeze(np.array(pd.read_feather('lbl_all_trial3.feather')))\n",
    "\n",
    "all_ds = TextDataset(all_clas, all_labels)\n",
    "all_dl = DataLoader(all_ds, bs, transpose=True, num_workers=1, pad_idx=1)\n",
    "\n",
    "md = ModelData('', None, None, all_dl)\n",
    "\n",
    "learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)\n",
    "learn.load(\"clas_2_trial3_cont_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Score vectors with 52 elements for each obs.\n",
    "all_scores = learn.predict(is_test=True)\n",
    "# all_prob can be obtained by applying softmax function\n",
    "# predictions can be obtained by np.argmax()\n",
    "# Saving the results as dataframe for further diagnosis\n",
    "pd.DataFrame(all_scores, columns=itol.values.astype('str')).to_feather('Results/all_scores_trial3_cont.feather')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
